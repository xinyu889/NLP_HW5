{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "def is_chinese(string):\n",
    "    for ch in string:\n",
    "        if u'\\u4e00' <= ch <= u'\\u9fff':\n",
    "            return True\n",
    "\n",
    "    return False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from opencc import OpenCC\n",
    "ch = OpenCC('s2twp')\n",
    "def init():\n",
    "    lists = []\n",
    "    # reading the json file\n",
    "    with open('./translation2019zh/translation2019zh_train.json','r', encoding='utf-8') as dat_f:\n",
    "        data = []\n",
    "        for i,line in enumerate(dat_f):\n",
    "            data = json.loads(line)\n",
    "\n",
    "            if is_chinese(data['chinese']) == True:\n",
    "                data['chinese'] = ch.convert(data['chinese'])\n",
    "                lists.append(data)\n",
    "                if (i+1)%1000000 == 0:\n",
    "                    print(i+1)\n",
    "                if (i+1)%2000000 == 0:\n",
    "                    break\n",
    "        \n",
    "    # creating the dataframe\n",
    "    df = pd.DataFrame(lists)\n",
    "    # converted a file to csv\n",
    "    df.to_csv('datafile.csv', encoding='utf-8', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "init()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1000000\n",
      "2000000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('datafile.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# 分別生成中英文字典\n",
    "def en2id():\n",
    "    x = list(df['english'][:2000000])\n",
    "    en_vocab = set(''.join(x))\n",
    "    id2en = list(en_vocab)\n",
    "    en2id = {c:i for i,c in enumerate(id2en)}\n",
    "    #print('\\n英文字典:\\n', en2id)\n",
    "    return en2id\n",
    "\n",
    "def ch2id():\n",
    "    y = list(df['chinese'][:2000000])\n",
    "    ch_vocab = set(''.join(y))\n",
    "    id2ch = list(ch_vocab)\n",
    "    ch2id = {c:i for i,c in enumerate(id2ch)}\n",
    "    #print('\\n中文字典共計\\n:', ch2id)\n",
    "    return ch2id\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "en2id = en2id()\n",
    "ch2id = ch2id()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import numpy as np\n",
    "def encoder_input_data(en2id,ch2id):\n",
    "    x = list(df['english'][:2000000])\n",
    "    y = list(df['chinese'][:2000000])\n",
    "\n",
    "    en_num_data = [[en2id[en] for en in line ] for line in x]\n",
    "    ch_num_data = [[ch2id[ch] for ch in line] for line in y]\n",
    "\n",
    "    print('char:', x[1])\n",
    "    print('index:', en_num_data[1])\n",
    "\n",
    "    # 獲取輸入輸出端的最大長度\n",
    "    max_encoder_seq_length = max([len(txt) for txt in en_num_data])\n",
    "    print('max encoder length:', max_encoder_seq_length)\n",
    "\n",
    "    # 將數據進行onehot處理\n",
    "    encoder_input_data = np.zeros((len(en_num_data), max_encoder_seq_length, len(en2id)), dtype='float32')\n",
    "\n",
    "    for i in range(len(ch_num_data)):\n",
    "        for t, j in enumerate(en_num_data[i]):\n",
    "            encoder_input_data[i, t, j] = 1.\n",
    "            \n",
    "    print('index data:\\n', en_num_data[1])\n",
    "    print('one hot data:\\n', encoder_input_data[1])\n",
    "    return encoder_input_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "\n",
    "def decoder_input_data(ch2id):\n",
    "    y = list(df['chinese'][:2000000])\n",
    "\n",
    "    ch_num_data = [[ch2id[ch] for ch in line] for line in y]\n",
    "\n",
    "    print('char:', y[1])\n",
    "    print('index:', ch_num_data[1])\n",
    "\n",
    "    # 獲取輸入輸出端的最大長度\n",
    "    max_decoder_seq_length = max([len(txt) for txt in ch_num_data])\n",
    "    print('max decoder length:', max_decoder_seq_length)\n",
    "\n",
    "    # 將數據進行onehot處理\n",
    "    decoder_input_data = np.zeros((len(ch_num_data), max_decoder_seq_length, len(ch2id)), dtype='float32')\n",
    "\n",
    "    for i in range(len(ch_num_data)):\n",
    "        for t, j in enumerate(ch_num_data[i]):\n",
    "            decoder_input_data[i, t, j] = 1.\n",
    "            \n",
    "    print('index data:\\n', ch_num_data[1])\n",
    "    print('one hot data:\\n', decoder_input_data[1])\n",
    "    return decoder_input_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def decoder_target_data(ch2id):\n",
    "    y = list(df['chinese'][:2000000])\n",
    "    de_num_data = [[ch2id[ch] for ch in line][1:] for line in y]\n",
    "    ch_num_data = [[ch2id[ch] for ch in line] for line in y]\n",
    "    # 獲取輸入輸出端的最大長度\n",
    "    max_decoder_seq_length = max([len(txt) for txt in ch_num_data])\n",
    "    print('max decoder length:', max_decoder_seq_length)\n",
    "    decoder_target_data = np.zeros((len(ch_num_data), max_decoder_seq_length, len(ch2id)), dtype='float32')\n",
    "\n",
    "    for i in range(len(ch_num_data)):\n",
    "        for t, j in enumerate(de_num_data[i]):\n",
    "            decoder_target_data[i, t, j] = 1.\n",
    "            \n",
    "    return decoder_target_data\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "encoder_input_data = encoder_input_data(en2id,ch2id)\n",
    "decoder_input_data = decoder_input_data(ch2id)\n",
    "decoder_target_data = decoder_target_data(ch2id)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "char: He calls the Green Book, his book of teachings, “the new gospel.\n",
      "index: [197, 182, 235, 98, 247, 94, 94, 141, 235, 213, 168, 182, 235, 276, 304, 182, 182, 130, 235, 236, 119, 119, 169, 301, 235, 168, 29, 141, 235, 84, 119, 119, 169, 235, 119, 95, 235, 213, 182, 247, 98, 168, 29, 130, 125, 141, 301, 235, 248, 213, 168, 182, 235, 130, 182, 147, 235, 125, 119, 141, 52, 182, 94, 155]\n",
      "max encoder length: 419\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "EN_VOCAB_SIZE = len(en2id)\n",
    "CH_VOCAB_SIZE = len(ch2id)\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "LEARNING_RATE = 0.003\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 200"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e4b7846fcbc8ecfda520f9b185e4660bd78f1cc9654f339f017f4644e0a3a534"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}