{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tensorflow.keras import optimizers\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from IPython.display import clear_output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "def is_chinese(string):\n",
    "    for ch in string:\n",
    "        if u'\\u4e00' <= ch <= u'\\u9fff':\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def init():\n",
    "    lists = []\n",
    "    # reading the json file\n",
    "    with open('./translation2019zh/translation2019zh_train.json','r', encoding='utf-8') as dat_f:\n",
    "        data = []\n",
    "        for i,line in enumerate(dat_f):\n",
    "            data = json.loads(line)\n",
    "\n",
    "            if is_chinese(data['chinese']) == True:\n",
    "                #data['chinese'] = ch.convert(data['chinese'])\n",
    "                if len(data['chinese'])<5:\n",
    "                    lists.append(data)\n",
    "                if (len(lists)+1)%30 == 0:\n",
    "                    print(len(lists)+1)\n",
    "                    break\n",
    "\n",
    "    # creating the dataframe\n",
    "    df = pd.DataFrame(lists)\n",
    "    # converted a file to csv\n",
    "    df.to_csv('datafile.csv', encoding='utf-8', index=False)\n",
    "\n",
    "init()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "30\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('datafile.csv')\n",
    "df['chinese'] = df['chinese'].apply(lambda x: '@' + x + '。')\n",
    "print(df[:1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                           english chinese\n",
      "0  Erhai Lake （in Yunnan Province）    @洱海。\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "en_data = df.english.values.tolist()#英文句子列表\n",
    "ch_data = df.chinese.values.tolist()#中文句子列表\n",
    "\n",
    "#确定中英文各自包含的字符。df.unique()直接取sum可将unique数组中的各个句子拼接成一个长句子\n",
    "# 分別生成中英文字典\n",
    "en_vocab = set(''.join(en_data))\n",
    "id2en = list(en_vocab)\n",
    "en2id = {c:i for i,c in enumerate(id2en)}\n",
    "\n",
    "# 分別生成中英文字典\n",
    "ch_vocab = set(''.join(ch_data))\n",
    "id2ch = list(ch_vocab)\n",
    "ch2id = {c:i for i,c in enumerate(id2ch)}\n",
    "\n",
    "print('\\n英文字典:\\n', en2id)\n",
    "print('\\n中文字典共計\\n:', ch2id)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "英文字典:\n",
      " {'o': 0, 'j': 1, 'S': 2, 'W': 3, 'w': 4, ':': 5, 'K': 6, 'D': 7, 'B': 8, 'f': 9, 'C': 10, 'F': 11, ' ': 12, 'g': 13, 'T': 14, 'x': 15, 'n': 16, '.': 17, 'u': 18, 'L': 19, 'z': 20, 'r': 21, 'd': 22, \"'\": 23, 'k': 24, 'l': 25, '=': 26, 'E': 27, 'e': 28, 'a': 29, 'A': 30, 'U': 31, 'I': 32, 'q': 33, ',': 34, 'm': 35, 't': 36, '（': 37, 'V': 38, 'N': 39, 'i': 40, '!': 41, 'Y': 42, 'v': 43, 'h': 44, 'p': 45, 'y': 46, '\"': 47, '-': 48, '）': 49, ';': 50, '?': 51, 'H': 52, 's': 53, 'c': 54, 'b': 55, 'M': 56, 'P': 57}\n",
      "\n",
      "中文字典共計\n",
      ": {'切': 0, '东': 1, '盒': 2, '虾': 3, '核': 4, '鲜': 5, '你': 6, '尼': 7, '京': 8, '楼': 9, '@': 10, '什': 11, '剪': 12, '街': 13, '证': 14, '抱': 15, '拼': 16, '线': 17, '二': 18, '了': 19, '密': 20, '目': 21, '请': 22, '片': 23, '是': 24, '该': 25, '印': 26, '手': 27, '机': 28, '刷': 29, '篮': 30, '蒸': 31, '于': 32, '保': 33, '？': 34, '海': 35, '紧': 36, '名': 37, '；': 38, '包': 39, '骨': 40, '偶': 41, '学': 42, '灌': 43, '坐': 44, '吧': 45, '毛': 46, '洱': 47, '思': 48, '厂': 49, '换': 50, '结': 51, '喝': 52, '很': 53, '先': 54, '在': 55, '气': 56, '花': 57, '肯': 58, '水': 59, '魁': 60, '梧': 61, '浴': 62, '标': 63, '势': 64, '我': 65, '做': 66, '车': 67, '.': 68, '痛': 69, '胃': 70, '化': 71, '草': 72, '开': 73, '塔': 74, '么': 75, '割': 76, '。': 77, '大': 78, '封': 79}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "# 利用字典，映射數據\n",
    "en_num_data = [[en2id[en] for en in line ] for line in en_data]\n",
    "ch_num_data = [[ch2id[ch] for ch in line] for line in ch_data]\n",
    "de_num_data = [[ch2id[ch] for ch in line][1:] for line in ch_data]\n",
    "\n",
    "print('char:', en_data[1])\n",
    "print('index:', en_num_data[1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "char: I promise\".\n",
      "index: [32, 12, 45, 21, 0, 35, 40, 53, 28, 47, 17]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "import numpy as np\n",
    "\n",
    "# 獲取輸入輸出端的最大長度\n",
    "max_encoder_seq_length = max([len(txt) for txt in en_num_data])\n",
    "max_decoder_seq_length = max([len(txt) for txt in ch_num_data])\n",
    "print('max encoder length:', max_encoder_seq_length)\n",
    "print('max decoder length:', max_decoder_seq_length)\n",
    "\n",
    "# 將數據進行onehot處理\n",
    "encoder_input_data = np.zeros((len(en_num_data), max_encoder_seq_length, len(en2id)), dtype='float32')\n",
    "decoder_input_data = np.zeros((len(ch_num_data), max_decoder_seq_length, len(ch2id)), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(ch_num_data), max_decoder_seq_length, len(ch2id)), dtype='float32')\n",
    "\n",
    "for i in range(len(ch_num_data)):\n",
    "    for t, j in enumerate(en_num_data[i]):\n",
    "        encoder_input_data[i, t, j] = 1.\n",
    "    for t, j in enumerate(ch_num_data[i]):\n",
    "        decoder_input_data[i, t, j] = 1.\n",
    "    for t, j in enumerate(de_num_data[i]):\n",
    "        decoder_target_data[i, t, j] = 1.\n",
    "\n",
    "print('index data:\\n', en_num_data[1])\n",
    "print('one hot data:\\n', encoder_input_data[1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "max encoder length: 41\n",
      "max decoder length: 6\n",
      "index data:\n",
      " [32, 12, 45, 21, 0, 35, 40, 53, 28, 47, 17]\n",
      "one hot data:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "# =======預定義模型參數========\n",
    "EN_VOCAB_SIZE = len(en2id)\n",
    "CH_VOCAB_SIZE = len(ch2id)\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "LEARNING_RATE = 0.0249\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 250"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "# ======================================keras model==================================\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# ==============encoder=============\n",
    "encoder_inputs = Input(shape=(None, EN_VOCAB_SIZE))\n",
    "#emb_inp = Embedding(output_dim=HIDDEN_SIZE, input_dim=EN_VOCAB_SIZE)(encoder_inputs)\n",
    "encoder_h1, encoder_state_h1, encoder_state_c1 = LSTM(HIDDEN_SIZE, return_sequences=True, return_state=True)(encoder_inputs)\n",
    "encoder_h2, encoder_state_h2, encoder_state_c2 = LSTM(HIDDEN_SIZE, return_state=True)(encoder_h1)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "# ==============decoder=============\n",
    "decoder_inputs = Input(shape=(None, CH_VOCAB_SIZE))\n",
    "\n",
    "#emb_target = Embedding(output_dim=HIDDEN_SIZE, input_dim=CH_VOCAB_SIZE, mask_zero=True)(decoder_inputs)\n",
    "lstm1 = LSTM(HIDDEN_SIZE, return_sequences=True, return_state=True)\n",
    "lstm2 = LSTM(HIDDEN_SIZE, return_sequences=True, return_state=True)\n",
    "decoder_dense = Dense(CH_VOCAB_SIZE, activation='softmax')\n",
    "\n",
    "decoder_h1, _, _ = lstm1(decoder_inputs, initial_state=[encoder_state_h1, encoder_state_c1])\n",
    "decoder_h2, _, _ = lstm2(decoder_h1, initial_state=[encoder_state_h2, encoder_state_c2])\n",
    "decoder_outputs = decoder_dense(decoder_h2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "opt = Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_split=0.)\n",
    "\n",
    "# Save model\n",
    "model.save('s2s.h5')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_20\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_33 (InputLayer)           [(None, None, 58)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_34 (InputLayer)           [(None, None, 80)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_20 (LSTM)                  [(None, None, 256),  322560      input_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_22 (LSTM)                  [(None, None, 256),  345088      input_34[0][0]                   \n",
      "                                                                 lstm_20[0][1]                    \n",
      "                                                                 lstm_20[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_21 (LSTM)                  [(None, 256), (None, 525312      lstm_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_23 (LSTM)                  [(None, None, 256),  525312      lstm_22[0][0]                    \n",
      "                                                                 lstm_21[0][1]                    \n",
      "                                                                 lstm_21[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, None, 80)     20560       lstm_23[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,738,832\n",
      "Trainable params: 1,738,832\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/250\n",
      "1/1 [==============================] - 6s 6s/step - loss: 3.5020 - accuracy: 0.0172\n",
      "Epoch 2/250\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 3.0838 - accuracy: 0.2701\n",
      "Epoch 3/250\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.9153 - accuracy: 0.0115\n",
      "Epoch 4/250\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 4.1121 - accuracy: 0.2701\n",
      "Epoch 5/250\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 3.2046 - accuracy: 0.0345\n",
      "Epoch 6/250\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.7625 - accuracy: 0.2701\n",
      "Epoch 7/250\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.6440 - accuracy: 0.2701\n",
      "Epoch 8/250\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 2.5701 - accuracy: 0.2701\n",
      "Epoch 9/250\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 2.4189 - accuracy: 0.2644\n",
      "Epoch 10/250\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 2.2170 - accuracy: 0.2759\n",
      "Epoch 11/250\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 2.0947 - accuracy: 0.2874\n",
      "Epoch 12/250\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 2.0486 - accuracy: 0.2816\n",
      "Epoch 13/250\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 1.9633 - accuracy: 0.2816\n",
      "Epoch 14/250\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 1.9278 - accuracy: 0.2816\n",
      "Epoch 15/250\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 1.9221 - accuracy: 0.2759\n",
      "Epoch 16/250\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 1.9168 - accuracy: 0.2874\n",
      "Epoch 17/250\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 1.9254 - accuracy: 0.2816\n",
      "Epoch 18/250\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 1.9203 - accuracy: 0.2759\n",
      "Epoch 19/250\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 1.8962 - accuracy: 0.2816\n",
      "Epoch 20/250\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 1.8730 - accuracy: 0.2816\n",
      "Epoch 21/250\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 1.8574 - accuracy: 0.2816\n",
      "Epoch 22/250\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 1.8503 - accuracy: 0.2759\n",
      "Epoch 23/250\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 1.8727 - accuracy: 0.2989\n",
      "Epoch 24/250\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 1.8415 - accuracy: 0.2931\n",
      "Epoch 25/250\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 1.8187 - accuracy: 0.2931\n",
      "Epoch 26/250\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 1.8109 - accuracy: 0.2989\n",
      "Epoch 27/250\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 1.8229 - accuracy: 0.2759\n",
      "Epoch 28/250\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 1.8001 - accuracy: 0.3218\n",
      "Epoch 29/250\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 1.7783 - accuracy: 0.3218\n",
      "Epoch 30/250\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 1.7621 - accuracy: 0.3046\n",
      "Epoch 31/250\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 1.7489 - accuracy: 0.3218\n",
      "Epoch 32/250\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 1.7347 - accuracy: 0.3046\n",
      "Epoch 33/250\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 1.7180 - accuracy: 0.3448\n",
      "Epoch 34/250\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 1.6723 - accuracy: 0.3333\n",
      "Epoch 35/250\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 1.6854 - accuracy: 0.3103\n",
      "Epoch 36/250\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 1.7679 - accuracy: 0.2989\n",
      "Epoch 37/250\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 1.6255 - accuracy: 0.3391\n",
      "Epoch 38/250\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 1.7900 - accuracy: 0.2931\n",
      "Epoch 39/250\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 1.6174 - accuracy: 0.3218\n",
      "Epoch 40/250\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 1.7649 - accuracy: 0.3046\n",
      "Epoch 41/250\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 1.6531 - accuracy: 0.3448\n",
      "Epoch 42/250\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 1.6119 - accuracy: 0.3391\n",
      "Epoch 43/250\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 1.6377 - accuracy: 0.2989\n",
      "Epoch 44/250\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 1.5419 - accuracy: 0.3506\n",
      "Epoch 45/250\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 1.5198 - accuracy: 0.3563\n",
      "Epoch 46/250\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 1.5234 - accuracy: 0.3678\n",
      "Epoch 47/250\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 1.4582 - accuracy: 0.3678\n",
      "Epoch 48/250\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 1.4637 - accuracy: 0.3621\n",
      "Epoch 49/250\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 1.4062 - accuracy: 0.3851\n",
      "Epoch 50/250\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 1.3747 - accuracy: 0.3851\n",
      "Epoch 51/250\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 1.3770 - accuracy: 0.3793\n",
      "Epoch 52/250\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 1.3512 - accuracy: 0.3851\n",
      "Epoch 53/250\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 1.3476 - accuracy: 0.3793\n",
      "Epoch 54/250\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 1.2857 - accuracy: 0.4080\n",
      "Epoch 55/250\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 1.2683 - accuracy: 0.4023\n",
      "Epoch 56/250\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 1.2427 - accuracy: 0.4195\n",
      "Epoch 57/250\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 1.2250 - accuracy: 0.4195\n",
      "Epoch 58/250\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 1.2030 - accuracy: 0.4368\n",
      "Epoch 59/250\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 1.1719 - accuracy: 0.4253\n",
      "Epoch 60/250\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 1.1418 - accuracy: 0.4425\n",
      "Epoch 61/250\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 1.1466 - accuracy: 0.4540\n",
      "Epoch 62/250\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 1.1978 - accuracy: 0.4138\n",
      "Epoch 63/250\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 1.2508 - accuracy: 0.4023\n",
      "Epoch 64/250\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 1.2006 - accuracy: 0.4080\n",
      "Epoch 65/250\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 1.1117 - accuracy: 0.4310\n",
      "Epoch 66/250\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 1.0992 - accuracy: 0.4540\n",
      "Epoch 67/250\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 1.0944 - accuracy: 0.4253\n",
      "Epoch 68/250\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 1.0526 - accuracy: 0.4655\n",
      "Epoch 69/250\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 1.0426 - accuracy: 0.4713\n",
      "Epoch 70/250\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 1.0008 - accuracy: 0.4540\n",
      "Epoch 71/250\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.9777 - accuracy: 0.4598\n",
      "Epoch 72/250\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.9510 - accuracy: 0.4655\n",
      "Epoch 73/250\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.9265 - accuracy: 0.5230\n",
      "Epoch 74/250\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 1.5440 - accuracy: 0.4425\n",
      "Epoch 75/250\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 1.4435 - accuracy: 0.4023\n",
      "Epoch 76/250\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 1.3300 - accuracy: 0.4483\n",
      "Epoch 77/250\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 1.2245 - accuracy: 0.4080\n",
      "Epoch 78/250\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 1.1737 - accuracy: 0.4483\n",
      "Epoch 79/250\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 1.1257 - accuracy: 0.4138\n",
      "Epoch 80/250\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 1.0425 - accuracy: 0.4770\n",
      "Epoch 81/250\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 1.0892 - accuracy: 0.4483\n",
      "Epoch 82/250\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 1.0039 - accuracy: 0.5000\n",
      "Epoch 83/250\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 1.0665 - accuracy: 0.4483\n",
      "Epoch 84/250\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 1.1650 - accuracy: 0.4943\n",
      "Epoch 85/250\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 1.1533 - accuracy: 0.4425\n",
      "Epoch 86/250\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 1.0137 - accuracy: 0.4713\n",
      "Epoch 87/250\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.9792 - accuracy: 0.4598\n",
      "Epoch 88/250\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.9317 - accuracy: 0.4943\n",
      "Epoch 89/250\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.9115 - accuracy: 0.4655\n",
      "Epoch 90/250\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.8516 - accuracy: 0.5172\n",
      "Epoch 91/250\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.8120 - accuracy: 0.5460\n",
      "Epoch 92/250\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.7805 - accuracy: 0.5690\n",
      "Epoch 93/250\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.7631 - accuracy: 0.5287\n",
      "Epoch 94/250\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.7261 - accuracy: 0.5517\n",
      "Epoch 95/250\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.7206 - accuracy: 0.5805\n",
      "Epoch 96/250\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.6920 - accuracy: 0.5862\n",
      "Epoch 97/250\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.6736 - accuracy: 0.5690\n",
      "Epoch 98/250\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.6530 - accuracy: 0.5920\n",
      "Epoch 99/250\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.6416 - accuracy: 0.5920\n",
      "Epoch 100/250\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.6281 - accuracy: 0.5977\n",
      "Epoch 101/250\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.6088 - accuracy: 0.5977\n",
      "Epoch 102/250\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.5952 - accuracy: 0.6149\n",
      "Epoch 103/250\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.5839 - accuracy: 0.6034\n",
      "Epoch 104/250\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.5700 - accuracy: 0.5920\n",
      "Epoch 105/250\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.5575 - accuracy: 0.6034\n",
      "Epoch 106/250\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.5478 - accuracy: 0.6034\n",
      "Epoch 107/250\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.5361 - accuracy: 0.6149\n",
      "Epoch 108/250\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.5240 - accuracy: 0.6264\n",
      "Epoch 109/250\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.5136 - accuracy: 0.6264\n",
      "Epoch 110/250\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.5333 - accuracy: 0.6322\n",
      "Epoch 111/250\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.5125 - accuracy: 0.6264\n",
      "Epoch 112/250\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.5035 - accuracy: 0.6379\n",
      "Epoch 113/250\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.4888 - accuracy: 0.6379\n",
      "Epoch 114/250\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.4810 - accuracy: 0.6552\n",
      "Epoch 115/250\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.4691 - accuracy: 0.6379\n",
      "Epoch 116/250\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.4595 - accuracy: 0.6494\n",
      "Epoch 117/250\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.4479 - accuracy: 0.6379\n",
      "Epoch 118/250\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.4471 - accuracy: 0.6609\n",
      "Epoch 119/250\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.4567 - accuracy: 0.6437\n",
      "Epoch 120/250\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.5029 - accuracy: 0.6207\n",
      "Epoch 121/250\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.7336 - accuracy: 0.5172\n",
      "Epoch 122/250\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.8391 - accuracy: 0.4828\n",
      "Epoch 123/250\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.4819 - accuracy: 0.6264\n",
      "Epoch 124/250\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.7638 - accuracy: 0.5000\n",
      "Epoch 125/250\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.6264 - accuracy: 0.5690\n",
      "Epoch 126/250\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.9779 - accuracy: 0.4310\n",
      "Epoch 127/250\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.7565 - accuracy: 0.5287\n",
      "Epoch 128/250\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 1.0752 - accuracy: 0.4310\n",
      "Epoch 129/250\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 1.0361 - accuracy: 0.4885\n",
      "Epoch 130/250\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.9633 - accuracy: 0.4770\n",
      "Epoch 131/250\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.8623 - accuracy: 0.5057\n",
      "Epoch 132/250\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 1.0036 - accuracy: 0.5057\n",
      "Epoch 133/250\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.9966 - accuracy: 0.5230\n",
      "Epoch 134/250\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.8748 - accuracy: 0.5517\n",
      "Epoch 135/250\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.8075 - accuracy: 0.5747\n",
      "Epoch 136/250\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.7829 - accuracy: 0.5460\n",
      "Epoch 137/250\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.7276 - accuracy: 0.5862\n",
      "Epoch 138/250\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.8024 - accuracy: 0.5977\n",
      "Epoch 139/250\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.8126 - accuracy: 0.5460\n",
      "Epoch 140/250\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.9900 - accuracy: 0.4770\n",
      "Epoch 141/250\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.7284 - accuracy: 0.5632\n",
      "Epoch 142/250\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.9321 - accuracy: 0.4770\n",
      "Epoch 143/250\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.7408 - accuracy: 0.5575\n",
      "Epoch 144/250\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.7822 - accuracy: 0.5345\n",
      "Epoch 145/250\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.6361 - accuracy: 0.5862\n",
      "Epoch 146/250\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.6335 - accuracy: 0.5575\n",
      "Epoch 147/250\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.5961 - accuracy: 0.6092\n",
      "Epoch 148/250\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.5948 - accuracy: 0.6034\n",
      "Epoch 149/250\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.5985 - accuracy: 0.6149\n",
      "Epoch 150/250\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.5515 - accuracy: 0.6207\n",
      "Epoch 151/250\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.5114 - accuracy: 0.6494\n",
      "Epoch 152/250\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.4951 - accuracy: 0.6494\n",
      "Epoch 153/250\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.5066 - accuracy: 0.6034\n",
      "Epoch 154/250\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.5087 - accuracy: 0.6494\n",
      "Epoch 155/250\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.4884 - accuracy: 0.6437\n",
      "Epoch 156/250\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.5974 - accuracy: 0.6149\n",
      "Epoch 157/250\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.6358 - accuracy: 0.5862\n",
      "Epoch 158/250\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.5292 - accuracy: 0.6149\n",
      "Epoch 159/250\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.5105 - accuracy: 0.6379\n",
      "Epoch 160/250\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.5182 - accuracy: 0.6322\n",
      "Epoch 161/250\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.4744 - accuracy: 0.6494\n",
      "Epoch 162/250\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.4420 - accuracy: 0.6609\n",
      "Epoch 163/250\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.4363 - accuracy: 0.6667\n",
      "Epoch 164/250\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.4493 - accuracy: 0.6667\n",
      "Epoch 165/250\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.4312 - accuracy: 0.6724\n",
      "Epoch 166/250\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.3822 - accuracy: 0.6954\n",
      "Epoch 167/250\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.3770 - accuracy: 0.7011\n",
      "Epoch 168/250\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.3630 - accuracy: 0.7011\n",
      "Epoch 169/250\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.4031 - accuracy: 0.6667\n",
      "Epoch 170/250\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.3927 - accuracy: 0.6552\n",
      "Epoch 171/250\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.3502 - accuracy: 0.6839\n",
      "Epoch 172/250\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.3495 - accuracy: 0.6782\n",
      "Epoch 173/250\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.3572 - accuracy: 0.6897\n",
      "Epoch 174/250\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.3396 - accuracy: 0.6839\n",
      "Epoch 175/250\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3222 - accuracy: 0.7011\n",
      "Epoch 176/250\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.3162 - accuracy: 0.7126\n",
      "Epoch 177/250\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.3092 - accuracy: 0.7126\n",
      "Epoch 178/250\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.3042 - accuracy: 0.7126\n",
      "Epoch 179/250\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.2987 - accuracy: 0.7126\n",
      "Epoch 180/250\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.2923 - accuracy: 0.7184\n",
      "Epoch 181/250\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.2868 - accuracy: 0.7126\n",
      "Epoch 182/250\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.2812 - accuracy: 0.7126\n",
      "Epoch 183/250\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.2767 - accuracy: 0.7184\n",
      "Epoch 184/250\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.2722 - accuracy: 0.7126\n",
      "Epoch 185/250\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.2666 - accuracy: 0.7299\n",
      "Epoch 186/250\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.2607 - accuracy: 0.7299\n",
      "Epoch 187/250\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.2548 - accuracy: 0.7241\n",
      "Epoch 188/250\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.2533 - accuracy: 0.7241\n",
      "Epoch 189/250\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.2530 - accuracy: 0.7241\n",
      "Epoch 190/250\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.2467 - accuracy: 0.7356\n",
      "Epoch 191/250\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.2418 - accuracy: 0.7299\n",
      "Epoch 192/250\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.2390 - accuracy: 0.7356\n",
      "Epoch 193/250\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.2327 - accuracy: 0.7356\n",
      "Epoch 194/250\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.2315 - accuracy: 0.7356\n",
      "Epoch 195/250\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.2267 - accuracy: 0.7356\n",
      "Epoch 196/250\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.2221 - accuracy: 0.7356\n",
      "Epoch 197/250\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.2194 - accuracy: 0.7299\n",
      "Epoch 198/250\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.2156 - accuracy: 0.7471\n",
      "Epoch 199/250\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.2124 - accuracy: 0.7356\n",
      "Epoch 200/250\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.2091 - accuracy: 0.7414\n",
      "Epoch 201/250\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.2055 - accuracy: 0.7586\n",
      "Epoch 202/250\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.2026 - accuracy: 0.7471\n",
      "Epoch 203/250\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.1986 - accuracy: 0.7471\n",
      "Epoch 204/250\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.1949 - accuracy: 0.7414\n",
      "Epoch 205/250\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.1993 - accuracy: 0.7414\n",
      "Epoch 206/250\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.2270 - accuracy: 0.7299\n",
      "Epoch 207/250\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.2669 - accuracy: 0.7126\n",
      "Epoch 208/250\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.2280 - accuracy: 0.7414\n",
      "Epoch 209/250\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.2232 - accuracy: 0.7356\n",
      "Epoch 210/250\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.2080 - accuracy: 0.7356\n",
      "Epoch 211/250\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.2081 - accuracy: 0.7299\n",
      "Epoch 212/250\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.2046 - accuracy: 0.7414\n",
      "Epoch 213/250\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.1987 - accuracy: 0.7356\n",
      "Epoch 214/250\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.1941 - accuracy: 0.7356\n",
      "Epoch 215/250\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.1868 - accuracy: 0.7414\n",
      "Epoch 216/250\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.1848 - accuracy: 0.7356\n",
      "Epoch 217/250\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.1800 - accuracy: 0.7356\n",
      "Epoch 218/250\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.1837 - accuracy: 0.7356\n",
      "Epoch 219/250\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.1785 - accuracy: 0.7356\n",
      "Epoch 220/250\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.1818 - accuracy: 0.7414\n",
      "Epoch 221/250\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.1748 - accuracy: 0.7356\n",
      "Epoch 222/250\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.1676 - accuracy: 0.7471\n",
      "Epoch 223/250\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.1662 - accuracy: 0.7414\n",
      "Epoch 224/250\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.1665 - accuracy: 0.7414\n",
      "Epoch 225/250\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.1632 - accuracy: 0.7471\n",
      "Epoch 226/250\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.1549 - accuracy: 0.7471\n",
      "Epoch 227/250\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.1549 - accuracy: 0.7471\n",
      "Epoch 228/250\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.1498 - accuracy: 0.7471\n",
      "Epoch 229/250\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.1484 - accuracy: 0.7414\n",
      "Epoch 230/250\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.1450 - accuracy: 0.7586\n",
      "Epoch 231/250\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.1400 - accuracy: 0.7529\n",
      "Epoch 232/250\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.1398 - accuracy: 0.7471\n",
      "Epoch 233/250\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.1362 - accuracy: 0.7471\n",
      "Epoch 234/250\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.1328 - accuracy: 0.7471\n",
      "Epoch 235/250\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.1326 - accuracy: 0.7644\n",
      "Epoch 236/250\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.1281 - accuracy: 0.7586\n",
      "Epoch 237/250\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.1266 - accuracy: 0.7586\n",
      "Epoch 238/250\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.1245 - accuracy: 0.7586\n",
      "Epoch 239/250\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.1211 - accuracy: 0.7586\n",
      "Epoch 240/250\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.1201 - accuracy: 0.7586\n",
      "Epoch 241/250\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.1168 - accuracy: 0.7586\n",
      "Epoch 242/250\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.1156 - accuracy: 0.7586\n",
      "Epoch 243/250\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.1134 - accuracy: 0.7586\n",
      "Epoch 244/250\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.1121 - accuracy: 0.7701\n",
      "Epoch 245/250\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.1103 - accuracy: 0.7644\n",
      "Epoch 246/250\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.1090 - accuracy: 0.7644\n",
      "Epoch 247/250\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.1075 - accuracy: 0.7644\n",
      "Epoch 248/250\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.1060 - accuracy: 0.7644\n",
      "Epoch 249/250\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.1046 - accuracy: 0.7701\n",
      "Epoch 250/250\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.1032 - accuracy: 0.7644\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "# encoder模型和訓練相同\n",
    "encoder_model = Model(encoder_inputs, [encoder_state_h1, encoder_state_c1, encoder_state_h2, encoder_state_c2])\n",
    "\n",
    "# 預測模型中的decoder的初始化狀態需要傳入新的狀態\n",
    "decoder_state_input_h1 = Input(shape=(HIDDEN_SIZE,))\n",
    "decoder_state_input_c1 = Input(shape=(HIDDEN_SIZE,))\n",
    "decoder_state_input_h2 = Input(shape=(HIDDEN_SIZE,))\n",
    "decoder_state_input_c2 = Input(shape=(HIDDEN_SIZE,))\n",
    "\n",
    "# 使用傳入的值來初始化當前模型的輸入狀態\n",
    "decoder_h1, state_h1, state_c1 = lstm1(decoder_inputs, initial_state=[decoder_state_input_h1, decoder_state_input_c1])\n",
    "decoder_h2, state_h2, state_c2 = lstm2(decoder_h1, initial_state=[decoder_state_input_h2, decoder_state_input_c2])\n",
    "decoder_outputs = decoder_dense(decoder_h2)\n",
    "\n",
    "decoder_model = Model([decoder_inputs, decoder_state_input_h1, decoder_state_input_c1, decoder_state_input_h2, decoder_state_input_c2], \n",
    "                      [decoder_outputs, state_h1, state_c1, state_h2, state_c2])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "for k in range(0,10):\n",
    "    test_data = encoder_input_data[k:k+1]\n",
    "    h1, c1, h2, c2 = encoder_model.predict(test_data)\n",
    "    target_seq = np.zeros((1, 1, CH_VOCAB_SIZE))\n",
    "    target_seq[0, 0, ch2id['@']] = 1\n",
    "    outputs = []\n",
    "    while True:\n",
    "        output_tokens, h1, c1, h2, c2 = decoder_model.predict([target_seq, h1, c1, h2, c2])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        outputs.append(sampled_token_index)\n",
    "        target_seq = np.zeros((1, 1, CH_VOCAB_SIZE))\n",
    "        target_seq[0, 0, sampled_token_index] = 1\n",
    "        if sampled_token_index == ch2id['。'] or len(outputs) > 15:\n",
    "            break\n",
    "    \n",
    "    print(en_data[k])\n",
    "    print(''.join([id2ch[i] for i in outputs]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Erhai Lake （in Yunnan Province）\n",
      "洱海。\n",
      "I promise\".\n",
      "我保证。\n",
      "Steam bath;\n",
      "蒸气浴；。\n",
      "Package sealing.\n",
      "包密封。\n",
      "What street?\n",
      "什么街？。\n",
      "You must be very quiet. Hold tight to me.\n",
      "抱紧我。\n",
      "Frozen ocean shrimp;\n",
      "海虾；。\n",
      "Target Line;\n",
      "目标线；。\n",
      "Chemical potential.\n",
      "化学势。\n",
      "A:It's on the seccond floor.\n",
      "在二楼.。\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('3.7.6': pyenv)"
  },
  "interpreter": {
   "hash": "7fc41b1a7acd303dd356c32eae0bc8ee4149514fe8a099b2279ea3ac2a654e9b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}